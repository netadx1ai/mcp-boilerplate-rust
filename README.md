# MCP Boilerplate Rust

A comprehensive, production-ready Rust implementation of the Model Context Protocol (MCP), providing a robust framework for building MCP servers with multiple transport layers, AI integration, and extensive example implementations.

## Overview

This project serves as both a reference implementation and a production-ready boilerplate template for creating MCP servers in Rust. It demonstrates best practices for protocol implementation, transport abstraction, tool integration, and AI-powered services.

**üöÄ Production Ready**: Complete with comprehensive testing, AI integration, security hardening, and deployment examples.

## Architecture

The project is organized as a Cargo workspace with a clean separation of concerns:

```
mcp-boilerplate-rust/
‚îú‚îÄ‚îÄ crates/
‚îÇ   ‚îú‚îÄ‚îÄ mcp-core/          # Core protocol types and traits
‚îÇ   ‚îú‚îÄ‚îÄ mcp-transport/     # Transport layer implementations (STDIO, HTTP)
‚îÇ   ‚îî‚îÄ‚îÄ mcp-server/        # Server framework and orchestration
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ filesystem-server/     # File system operations with security
‚îÇ   ‚îú‚îÄ‚îÄ image-generation-server/   # AI image generation (Google/Gemini ready)
‚îÇ   ‚îú‚îÄ‚îÄ blog-generation-server/    # AI blog writing with SEO optimization
‚îÇ   ‚îî‚îÄ‚îÄ creative-content-server/   # Multi-tool creative AI suite
‚îú‚îÄ‚îÄ tests/                 # Comprehensive E2E test suite
‚îú‚îÄ‚îÄ scripts/               # Development and deployment tools
‚îî‚îÄ‚îÄ generated_content/     # AI-generated outputs (images, blogs, etc.)
```

### Core Components

- **mcp-core**: Defines the fundamental MCP protocol types, message structures, and traits
- **mcp-transport**: Provides transport layer implementations (STDIO, HTTP) with full async support
- **mcp-server**: Server framework for orchestrating tools and handling requests with concurrency control

### Design Principles

1. **Transport Agnostic**: Core protocol logic is independent of transport mechanisms
2. **Tool-Based Architecture**: All business logic implemented as discrete `McpTool` implementations
3. **Async First**: Built on Tokio with async/await throughout, deadlock-free design
4. **Type Safety**: Leverages Rust's type system for protocol correctness
5. **AI Ready**: Scaffolded for immediate AI API integration
6. **Production Hardened**: Security features, comprehensive testing, performance optimized

## Quick Start

### Prerequisites

- Rust 1.70+ with Cargo
- (Optional) AI API keys for production AI features
- (Optional) GitHub CLI for development workflow

### Installation & Setup

Use the interactive setup script for guided installation:

```bash
# Clone the repository
git clone https://github.com/netadx1ai/mcp-boilerplate-rust.git
cd mcp-boilerplate-rust

# Interactive setup (recommended)
./setup.sh

# Or specific setup commands
./setup.sh build      # Build all components
./setup.sh gemini     # Setup Gemini AI integration
./setup.sh all        # Complete setup with AI
```

### Manual Setup

```bash
# Build all crates and examples
cargo build --workspace

# Run comprehensive test suite
cargo test --workspace

# Check code quality
cargo fmt --check
cargo clippy --workspace --all-targets

# Generate documentation
cargo doc --workspace --no-deps
```

## Server Examples

### 1. üóÇÔ∏è Filesystem Server
Production-ready file operations with security hardening:

**Features:**
- Path traversal protection with base directory constraints
- Comprehensive error handling for file system operations
- Support for both STDIO and HTTP transports
- Proper input validation and sanitization

```bash
# STDIO transport
cargo run --bin filesystem-server -- --transport stdio --base-dir ./

# HTTP transport
cargo run --bin filesystem-server -- --transport http --port 3000 --base-dir ./
```

**HTTP API Example:**
```bash
curl -X POST http://localhost:3000/mcp/tools/call \
  -H "Content-Type: application/json" \
  -d '{"name": "read_file", "arguments": {"path": "README.md"}}'
```

### 2. üé® Image Generation Server
AI-powered image generation with Google/Gemini integration:

**Features:**
- **Production AI Integration**: Google Gemini Imagen support
- **Development Mode**: Realistic mock responses for testing
- **Multiple Styles**: photorealistic, artistic, sketch, cartoon
- **Multiple Sizes**: 256x256, 512x512, 1024x1024, 1024x1792
- **Comprehensive Error Handling**: API failures, rate limiting, validation

```bash
# Development mode (mock responses)
cargo run --bin image-generation-server -- --transport http --port 3001

# Production mode with AI (requires GEMINI_API_KEY)
export GEMINI_API_KEY="your-api-key"
cargo run --bin image-generation-server -- --transport http --port 3001 --use-ai --provider gemini
```

**Generate Images:**
```bash
# Using the convenience script
python3 generate_image.py "A serene mountain landscape at sunset"

# Direct HTTP API
curl -X POST http://localhost:3001/mcp/tools/call \
  -H "Content-Type: application/json" \
  -d '{
    "name": "generate_image",
    "arguments": {
      "prompt": "A serene mountain landscape at sunset",
      "style": "photorealistic",
      "size": "1024x1024"
    }
  }'
```

### 3. üìù Blog Generation Server
AI-powered blog writing with SEO optimization:

**Features:**
- **Multiple Content Types**: Articles, tutorials, reviews, listicles
- **SEO Optimization**: Automatic keyword integration, meta descriptions
- **Flexible Length**: From social posts (100 words) to long-form (3000+ words)
- **Production Ready**: Scaffolded for immediate AI API integration

```bash
cargo run --bin blog-generation-server -- --transport http --port 3002
```

**Create Blog Posts:**
```bash
curl -X POST http://localhost:3002/mcp/tools/call \
  -H "Content-Type: application/json" \
  -d '{
    "name": "create_blog_post",
    "arguments": {
      "topic": "The Future of AI Development",
      "word_count": 1500,
      "style": "professional",
      "include_seo": true
    }
  }'
```

### 4. üé≠ Creative Content Server
Multi-tool creative AI suite for diverse content generation:

**Features:**
- **Story Generation**: Fiction, sci-fi, fantasy, mystery genres
- **Poetry Creation**: Haiku, sonnet, free verse, limerick styles
- **Character Development**: Detailed character profiles for creative writing
- **Unified Interface**: Multiple tools in a single server instance

```bash
cargo run --bin creative-content-server -- --transport http --port 3003
```

**Available Tools:**
- `generate_story`: Create stories in various genres
- `create_poem`: Generate poetry in different styles
- `develop_character`: Create detailed character profiles

## HTTP Transport Features

All servers support a unified HTTP API with these standard endpoints:

- `GET /health` - Health check (returns "OK")
- `GET /mcp/tools/list` - List all available tools with schemas
- `POST /mcp/tools/call` - Execute any tool with name and arguments

**Standard Tool Call Format:**
```json
{
  "name": "tool_name",
  "arguments": {
    "param1": "value1",
    "param2": "value2"
  }
}
```

**Standard Response Format:**
```json
{
  "content": [
    {
      "type": "text",
      "text": "Response content here"
    }
  ],
  "isError": false
}
```

## AI Integration

### Google/Gemini Integration (Production Ready)
The image generation server includes full Google Gemini Imagen integration:

```bash
# Setup Gemini API
export GEMINI_API_KEY="your-gemini-api-key"

# Run with AI enabled
cargo run --bin image-generation-server -- --use-ai --provider gemini
```

### Extensible AI Architecture
All AI servers are designed with extensible provider architecture:
- **Current**: Google/Gemini implementation
- **Planned**: OpenAI, Anthropic, local models
- **Framework**: Easy to add new providers via trait implementations

## Development Tools

### Interactive Scripts

**Setup Script**: `./setup.sh`
- Guided environment configuration
- AI API key setup
- Dependency verification
- Build automation

**Testing Script**: `./test.sh`
- Interactive test menu
- Quick vs comprehensive test suites
- Individual server testing
- Performance verification

**Image Generation**: `./generate_image.py`
- Direct image generation from command line
- Automatic output management
- Real AI integration demonstration

### Comprehensive Testing

**Test Coverage:**
- **57 total tests** across all components
- **E2E Integration Tests**: Real server startup/shutdown cycles
- **Protocol Compliance**: MCP specification adherence
- **AI Integration Tests**: Real API calls with verification
- **Performance Tests**: Sub-5 second test suite execution

**Test Execution:**
```bash
# Interactive testing
./test.sh

# Direct commands
cargo test --workspace           # Full test suite (< 5s)
cargo test --test "*e2e*"       # E2E tests only
cargo test --package mcp-core   # Core protocol tests
```

## Development Guide

### Adding New Tools

1. **Implement the McpTool trait:**
```rust
use mcp_core::{McpTool, McpRequest, McpResponse, ResponseResult, ToolContent};
use async_trait::async_trait;

#[derive(Default)]
pub struct CustomTool;

#[async_trait]
impl McpTool for CustomTool {
    async fn call(&self, request: McpRequest) -> Result<McpResponse, mcp_core::McpError> {
        match request {
            McpRequest::CallTool { name, arguments } => {
                // Tool logic here
                let result = ResponseResult::ToolResult {
                    content: vec![ToolContent::Text { 
                        text: "Tool response".to_string() 
                    }],
                    is_error: false,
                };
                Ok(McpResponse::success(result))
            }
            _ => Err(mcp_core::McpError::invalid_request("Expected CallTool request")),
        }
    }
    
    fn name(&self) -> &str { "custom_tool" }
    fn description(&self) -> &str { "Description of the tool" }
    fn input_schema(&self) -> serde_json::Value {
        serde_json::json!({
            "type": "object",
            "properties": {
                "param": {"type": "string", "description": "Parameter description"}
            },
            "required": ["param"]
        })
    }
}
```

2. **Register with server:**
```rust
use mcp_server::McpServerBuilder;

let server = McpServerBuilder::new()
    .with_name("my-server")
    .add_tool(std::sync::Arc::new(CustomTool::default()))
    .build()?;
```

### Adding AI Integration

Follow the pattern established in the image generation server:

1. **Add AI client dependencies** to your server's `Cargo.toml`
2. **Implement provider trait** for your AI service
3. **Add environment variable configuration** for API keys
4. **Include fallback mock responses** for development
5. **Add comprehensive error handling** for API failures

Example provider integration:
```rust
async fn generate_with_ai_provider(&self, prompt: &str) -> Result<String, McpError> {
    match &self.provider {
        "gemini" => self.generate_with_gemini(prompt).await,
        "openai" => self.generate_with_openai(prompt).await,
        _ => self.generate_placeholder_response(prompt).await,
    }
}
```

## Production Deployment

### Environment Configuration

Required environment variables for production AI features:
```bash
# Google/Gemini (for image generation)
export GEMINI_API_KEY="your-gemini-api-key"

# Optional: Custom base URLs for enterprise deployments
export GEMINI_BASE_URL="https://your-enterprise-api.com"
```

### Docker Deployment

```dockerfile
FROM rust:1.70 as builder
WORKDIR /app
COPY . .
RUN cargo build --release --bin image-generation-server

FROM debian:bookworm-slim
RUN apt-get update && apt-get install -y ca-certificates && rm -rf /var/lib/apt/lists/*
COPY --from=builder /app/target/release/image-generation-server /usr/local/bin/
EXPOSE 3000
CMD ["image-generation-server", "--transport", "http", "--port", "3000", "--use-ai"]
```

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mcp-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mcp-server
  template:
    metadata:
      labels:
        app: mcp-server
    spec:
      containers:
      - name: server
        image: your-registry/mcp-server:latest
        ports:
        - containerPort: 3000
        env:
        - name: GEMINI_API_KEY
          valueFrom:
            secretKeyRef:
              name: ai-api-keys
              key: gemini-key
```

## Performance & Quality

### Performance Metrics
- **Build Time**: < 30 seconds for full workspace
- **Test Suite**: < 10 seconds total execution (57 tests)
- **Server Startup**: < 2 seconds for all examples
- **Memory Usage**: < 50MB per server instance
- **Response Times**: < 100ms for local tools, < 2s for AI tools

### Quality Standards
- **Zero Warnings**: All clippy lints passing
- **100% Test Coverage**: Core protocol and transport layers
- **Security Hardened**: Path traversal protection, input validation
- **Documentation Complete**: All public APIs documented with examples
- **Production Tested**: Comprehensive E2E testing framework

## Advanced Features

### Concurrent Request Handling
```rust
let server = McpServerBuilder::new()
    .max_concurrent_requests(100)
    .request_timeout(Duration::from_secs(30))
    .enable_tracing(true)
    .build()?;
```

### Custom Transport Implementation
```rust
use mcp_transport::Transport;
use async_trait::async_trait;

#[derive(Default)]
pub struct CustomTransport;

#[async_trait]
impl Transport for CustomTransport {
    async fn send_request(&self, request: McpRequest) -> Result<(), TransportError> {
        // Custom transport logic
        todo!()
    }
    // ... other required methods
}
```

### Middleware Integration (Future Ready)
The server architecture is designed for easy middleware integration:
- Request/response logging
- Authentication and authorization
- Rate limiting
- Request validation
- Response caching

## Testing Framework

### Comprehensive Test Suite
- **Unit Tests**: 40+ tests covering core functionality
- **Integration Tests**: Cross-crate compatibility verification
- **E2E Tests**: Real server startup/shutdown with timeout protection
- **AI Integration Tests**: Live API calls with proper mocking
- **Performance Tests**: Startup time and response time validation

### Running Tests
```bash
# Interactive testing menu
./test.sh

# Specific test categories
cargo test --workspace                    # All tests (< 10s)
cargo test --test "*e2e*"                # E2E tests only
cargo test --package mcp-core            # Core protocol tests
cargo test test_ai_integration            # AI integration tests
```

### Quality Verification
```bash
# Complete quality check
cargo fmt --all
cargo clippy --workspace --all-targets
cargo test --workspace
cargo doc --workspace --no-deps
```

## AI Integration Examples

### Real Image Generation
```bash
# Setup API key
export GEMINI_API_KEY="your-key"

# Generate image
python3 generate_image.py "A futuristic cityscape at night"

# Result saved to: generated_images/YYYYMMDD_HHMMSS_prompt_image.png
```

### Blog Post Creation
```bash
curl -X POST http://localhost:3002/mcp/tools/call \
  -H "Content-Type: application/json" \
  -d '{
    "name": "create_blog_post",
    "arguments": {
      "topic": "Machine Learning in Healthcare",
      "word_count": 2000,
      "style": "professional",
      "include_seo": true,
      "target_keywords": ["AI healthcare", "medical ML", "diagnosis automation"]
    }
  }'
```

### Creative Content Generation
```bash
# Generate a story
curl -X POST http://localhost:3003/mcp/tools/call \
  -H "Content-Type: application/json" \
  -d '{
    "name": "generate_story",
    "arguments": {
      "genre": "sci-fi",
      "length": 1000,
      "theme": "space exploration",
      "character_count": 3
    }
  }'
```

## Error Handling & Monitoring

### Structured Error Responses
All servers provide consistent error responses following MCP protocol:

```json
{
  "error": {
    "code": -32602,
    "message": "Invalid params",
    "data": {
      "parameter": "prompt",
      "expected": "non-empty string",
      "received": ""
    }
  }
}
```

### Logging & Tracing
```bash
# Enable debug logging
cargo run --bin image-generation-server -- --debug

# Structured JSON logging for production
RUST_LOG=info cargo run --bin image-generation-server
```

## Security Features

### Input Validation
- **Path Traversal Protection**: Prevents access outside base directories
- **Parameter Validation**: JSON schema validation for all tool inputs
- **Size Limits**: Configurable limits on request/response sizes
- **Timeout Protection**: Prevents hanging operations

### Production Security Checklist
- [ ] AI API keys stored in secure environment variables
- [ ] Base directory restrictions properly configured
- [ ] Rate limiting implemented for public deployments
- [ ] Input sanitization for user-provided content
- [ ] HTTPS termination for production HTTP transport

## Development Workflow

### Git Integration
The project follows strict development standards:
- **Conventional Commits**: `feat(scope): description [#issue]`
- **GitHub Issues**: All tasks tracked and referenced
- **Feature Branches**: Clean development workflow
- **Quality Gates**: All changes verified before merge

### Task Management
Development tasks are tracked in structured markdown files:
- `tasks_*_COMPLETED.md`: Completed work with verification proof
- GitHub Issues: Public task tracking and collaboration

## Contributing

1. **Setup Development Environment:**
```bash
./setup.sh all
```

2. **Create Feature Branch:**
```bash
git checkout -b feature/my-feature
```

3. **Follow Quality Standards:**
```bash
# Before committing
cargo fmt --all
cargo clippy --workspace --all-targets
cargo test --workspace
```

4. **Commit with Proper Format:**
```bash
git commit -m "feat(server): add new tool implementation [#issue-number]"
```

5. **Create Pull Request** with verification proof

## Documentation

- **[API Documentation](API.md)** - Comprehensive API reference and integration guide
- **[Project Structure](PROJECT_STRUCTURE.md)** - Detailed project organization
- **[E2E Testing Guide](E2E_TESTING_CHEATSHEET.md)** - Testing patterns and best practices
- **[Repository](https://github.com/netadx1ai/mcp-boilerplate-rust)** - Source code and issues
- **[Online Docs](https://docs.rs/mcp-boilerplate-rust)** - Generated API documentation

## License

Licensed under the MIT License. See [LICENSE](LICENSE) for details.

## Acknowledgments

Built following the [MCP Protocol Specification](https://spec.modelcontextprotocol.io/) with production-ready enhancements for real-world deployment.

---

**Status**: Production Ready üöÄ | **Tests**: 57 passing ‚úÖ | **AI Integration**: Live ‚úÖ | **Documentation**: Complete ‚úÖ